{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mars-anogan-keras.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "gusMGRv8Dizz",
        "colab_type": "code",
        "outputId": "30dd9686-1d01-4716-ef6d-8170e7e58f61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        }
      },
      "source": [
        "from google.colab import drive\n",
        "#drive.mount('/content/gdrive')\n",
        "drive.mount('/content/gdrive', force_remount=False)\n",
        "GPATH=\"/content/gdrive/My Drive/aiml/OmdenaMars/\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHPSzaJEFcAJ",
        "colab_type": "text"
      },
      "source": [
        "# Download Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TtjeLS53DD-P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BASE_PATH=\"/content/data/\"\n",
        "TRAIN_NATURAL_PATH=BASE_PATH+\"train/\"+\"natural/\"\n",
        "TEST_NATURAL_PATH=BASE_PATH+\"test/\"+\"natural/\"\n",
        "TEST_TECHNO_PATH=BASE_PATH+\"test/\"+\"techno/\"\n",
        "TMP_PATH=BASE_PATH+\"tmp/\"\n",
        "WEIGHTS_PATH=GPATH+\"weights/anogan/\"\n",
        "RESULTS_PATH=BASE_PATH+\"results/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkqNsVcjFE0L",
        "colab_type": "code",
        "outputId": "a799e782-2ca9-4a22-f7f6-8f1d9368769b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "import pathlib\n",
        "from shutil import copyfile\n",
        "import os\n",
        "\n",
        "pathlib.Path(TRAIN_NATURAL_PATH).mkdir(parents=True, exist_ok=True)\n",
        "pathlib.Path(TEST_NATURAL_PATH).mkdir(parents=True, exist_ok=True)\n",
        "pathlib.Path(TEST_TECHNO_PATH).mkdir(parents=True, exist_ok=True)\n",
        "pathlib.Path(TMP_PATH).mkdir(parents=True, exist_ok=True)\n",
        "pathlib.Path(WEIGHTS_PATH).mkdir(parents=True, exist_ok=True)\n",
        "pathlib.Path(RESULTS_PATH).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "if not os.path.isfile(BASE_PATH+\"natural.tgz\"):\n",
        "    print(\"Copying natural.tgz to local folder\")\n",
        "    copyfile(GPATH+\"natural.tgz\", BASE_PATH+\"natural.tgz\")\n",
        "if not os.path.isfile(BASE_PATH+\"techno.tgz\"):\n",
        "    print(\"Copying techno.tgz to local folder\")\n",
        "    copyfile(GPATH+\"techno.tgz\", BASE_PATH+\"techno.tgz\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying natural.tgz to local folder\n",
            "Copying techno.tgz to local folder\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORP70B5PUx1p",
        "colab_type": "code",
        "outputId": "92801a1a-ee75-4322-ba29-1d10d2bda1be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Cleanup old directories\n",
        "!rm -rf /content/data/train/natural/\n",
        "!rm -rf /content/data/test/natural/\n",
        "!rm -rf /content/data/results\n",
        "!mkdir /content/data/train/natural/\n",
        "!mkdir /content/data/test/natural/\n",
        "!mkdir /content/data/results"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘/content/data/results’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8F7bL1iwF8lc",
        "colab_type": "code",
        "outputId": "7dca88cc-4c73-439e-af6c-0f16d2bf02cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        }
      },
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "print(\"Extracting techno.tgz\")\n",
        "os.system(\"tar -xzf \"+BASE_PATH+\"techno.tgz \"+\"-C \"+TMP_PATH)\n",
        "print(\"Extracting techno.tgz, Done.\")\n",
        "\n",
        "for dir in os.listdir(TMP_PATH+\"content/gdrive/My Drive/aiml/OmdenaMars/data/techno/\"):\n",
        "    print(\"Moving Files from:\", dir+\":\")\n",
        "    for file in os.listdir(TMP_PATH+\"content/gdrive/My Drive/aiml/OmdenaMars/data/techno/\"+dir):\n",
        "        #print(file)\n",
        "        shutil.move(TMP_PATH+\"content/gdrive/My Drive/aiml/OmdenaMars/data/techno/\"+dir+\"/\"+file, TEST_TECHNO_PATH+file)\n",
        "\n",
        "#cleanup temp dir\n",
        "shutil.rmtree(TMP_PATH+\"content\")\n",
        "\n",
        "print(\"Extracting natural.tgz\")\n",
        "os.system(\"tar -xzf \"+BASE_PATH+\"natural.tgz \"+\"-C \"+TMP_PATH)\n",
        "print(\"Extracting natural.tgz, Done.\")\n",
        "\n",
        "count = 204800\n",
        "print(\"Moving Natural File\")\n",
        "for file in os.listdir(TMP_PATH+\"content/gdrive/My Drive/aiml/OmdenaMars/data/natural/\"):\n",
        "    f1 = file.split(\"_\")\n",
        "    xVal = int(f1[2])\n",
        "    yVal = int(f1[4].split(\".\")[0])\n",
        "    if (xVal >= 8000) and (yVal>=5000):\n",
        "        if(xVal <= 15000) and (yVal <= 30000):\n",
        "            shutil.move(TMP_PATH+\"content/gdrive/My Drive/aiml/OmdenaMars/data/natural/\"+file, TRAIN_NATURAL_PATH+file)\n",
        "            count -= 1\n",
        "            if (count <= 0):\n",
        "                break\n",
        "\n",
        "print(\"Move 64 images from train/natural folder to test/natural folder\")\n",
        "count = 64\n",
        "for file in os.listdir(TRAIN_NATURAL_PATH):\n",
        "    shutil.move(TRAIN_NATURAL_PATH+file, TEST_NATURAL_PATH+file)\n",
        "    count -= 1\n",
        "    if count <= 0:\n",
        "        break\n",
        "print(\"Done\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting techno.tgz\n",
            "Extracting techno.tgz, Done.\n",
            "Moving Files from: Parachute:\n",
            "Moving Files from: Descent-Stage-Crash-Site:\n",
            "Moving Files from: 2-new-spots-spot2:\n",
            "Moving Files from: Curiosoity-Rover:\n",
            "Moving Files from: Heat-Sheild:\n",
            "Moving Files from: 2-new-spots-spot1:\n",
            "Extracting natural.tgz\n",
            "Extracting natural.tgz, Done.\n",
            "Moving Natural File\n",
            "Move 64 images from train/natural folder to test/natural folder\n",
            "Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8c-7qWaJDU2m",
        "colab_type": "text"
      },
      "source": [
        "# AnoGAN\n",
        "\n",
        "From: https://github.com/yjucho1/anoGAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FtTjcWKDDWIk",
        "colab_type": "code",
        "outputId": "56e77037-12a3-4125-b08e-1d62980e0973",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from __future__ import print_function\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Input, Reshape, Dense, Dropout, MaxPooling2D, Conv2D, Flatten\n",
        "from keras.layers import Conv2DTranspose, LeakyReLU\n",
        "from keras.layers.core import Activation\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.optimizers import Adam, RMSprop\n",
        "from keras import backend as K\n",
        "from keras import initializers\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import cv2\n",
        "import math\n",
        "import csv\n",
        "\n",
        "from keras.utils. generic_utils import Progbar\n",
        " \n",
        "### combine images for visualization\n",
        "def combine_images(generated_images):\n",
        "    num = generated_images.shape[0]\n",
        "    width = int(math.sqrt(num))\n",
        "    height = int(math.ceil(float(num)/width))\n",
        "    shape = generated_images.shape[1:4]\n",
        "    image = np.zeros((height*shape[0], width*shape[1], shape[2]),\n",
        "                     dtype=generated_images.dtype)\n",
        "    for index, img in enumerate(generated_images):\n",
        "        i = int(index/width)\n",
        "        j = index % width\n",
        "        image[i*shape[0]:(i+1)*shape[0], j*shape[1]:(j+1)*shape[1],:] = img[:, :, :]\n",
        "    return image\n",
        " \n",
        "### generator model define\n",
        "def generator_model():\n",
        "    inputs = Input((100,))\n",
        "    #M#fc1 = Dense(input_dim=10, units=128*7*7)(inputs)\n",
        "    fc1 = Dense(input_dim=10, units=128*32*32)(inputs)\n",
        "    fc1 = BatchNormalization()(fc1)\n",
        "    fc1 = LeakyReLU(0.2)(fc1)\n",
        "    #M#fc2 = Reshape((7, 7, 128), input_shape=(128*7*7,))(fc1)\n",
        "    fc2 = Reshape((32, 32, 128), input_shape=(128*64*64,))(fc1)\n",
        "    up1 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(fc2)\n",
        "    conv1 = Conv2D(64, (3, 3), padding='same')(up1)\n",
        "    conv1 = BatchNormalization()(conv1)\n",
        "    conv1 = Activation('relu')(conv1)\n",
        "    up2 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(conv1)\n",
        "    conv2 = Conv2D(1, (5, 5), padding='same')(up2)\n",
        "    outputs = Activation('tanh')(conv2)\n",
        "     \n",
        "    model = Model(inputs=[inputs], outputs=[outputs])\n",
        "    return model\n",
        " \n",
        "### discriminator model define\n",
        "def discriminator_model():\n",
        "    #M#inputs = Input((28, 28, 1))\n",
        "    inputs = Input((128, 128, 1))\n",
        "    conv1 = Conv2D(64, (5, 5), padding='same')(inputs)\n",
        "    conv1 = LeakyReLU(0.2)(conv1)\n",
        "    conv1 = Dropout(0.2)(conv1)\n",
        "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "    conv2 = Conv2D(128, (5, 5), padding='same')(pool1)\n",
        "    conv2 = LeakyReLU(0.2)(conv2)\n",
        "    conv2 = Dropout(0.2)(conv2)\n",
        "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
        "    \n",
        "    fc1 = Flatten()(pool2)\n",
        "    fc1 = Dense(1)(fc1)\n",
        "    outputs = Activation('sigmoid')(fc1)\n",
        "     \n",
        "    model = Model(inputs=[inputs], outputs=[outputs])\n",
        "    return model\n",
        " \n",
        "### d_on_g model for training generator\n",
        "def generator_containing_discriminator(g, d):\n",
        "    d.trainable = False\n",
        "    ganInput = Input(shape=(100,))\n",
        "    x = g(ganInput)\n",
        "    ganOutput = d(x)\n",
        "    gan = Model(inputs=ganInput, outputs=ganOutput)\n",
        "    #gan.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "    return gan\n",
        " \n",
        "def load_generator_model(best=False):\n",
        "    g = generator_model()\n",
        "    g_optim = RMSprop(lr=0.0002)\n",
        "    g.compile(loss='binary_crossentropy', optimizer=g_optim,metrics=['accuracy'])\n",
        "    if best == True:\n",
        "        g.load_weights(WEIGHTS_PATH+'generator.best.h5')\n",
        "    else:\n",
        "        g.load_weights(WEIGHTS_PATH+'generator.h5')\n",
        "    return g\n",
        "\n",
        "def load_discriminator_model(best=False):\n",
        "    d = discriminator_model()\n",
        "    d_optim = RMSprop(lr=0.0004)\n",
        "    d.compile(loss='binary_crossentropy', optimizer=d_optim, metrics=['accuracy'])\n",
        "    if best == True:\n",
        "        d.load_weights(WEIGHTS_PATH+'discriminator.best.h5')\n",
        "    else:\n",
        "        d.load_weights(WEIGHTS_PATH+'discriminator.h5')\n",
        "    return d\n",
        "\n",
        "def load_gan_model(g, d):\n",
        "    d_on_g = generator_containing_discriminator(g, d)\n",
        "    d_on_g.compile(loss='binary_crossentropy', optimizer=g_optim,metrics=['accuracy'])\n",
        "    d_on_g.load_weights(WEIGHTS_PATH+'gan.h5')\n",
        "    return g\n",
        "\n",
        "\n",
        "def load_model(best=False):\n",
        "    g = load_generator_model(best)\n",
        "    d = discriminator_model(best)\n",
        "    return g,d\n",
        "\n",
        "### train generator and discriminator\n",
        "def train(BATCH_SIZE, X_train_generator, reloadWeights=False):\n",
        "    \n",
        "    batch_size = X_train_generator.batch_size        \n",
        "    n_steps = int(X_train_generator.samples / batch_size)\n",
        "\n",
        "    ### model define\n",
        "    if reloadWeights == False:\n",
        "        d = discriminator_model()\n",
        "    else:\n",
        "        d= load_discriminator_model()\n",
        "        \n",
        "    if reloadWeights == False:\n",
        "        g = generator_model()\n",
        "    else:\n",
        "        g = load_generator_model()\n",
        "    \n",
        "    if reloadWeights == False:\n",
        "        d_on_g = generator_containing_discriminator(g, d)\n",
        "    else:\n",
        "        d_on_g = load_gan_model()\n",
        "        \n",
        "    d_optim = RMSprop(lr=0.0004)\n",
        "    g_optim = RMSprop(lr=0.0002)\n",
        "    \n",
        "    if reloadWeights == False:\n",
        "        #M#g.compile(loss='mse', optimizer=g_optim,metrics=['accuracy'])\n",
        "        g.compile(loss='binary_crossentropy', optimizer=g_optim,metrics=['accuracy'])\n",
        "    if reloadWeights == False:\n",
        "        #M#d_on_g.compile(loss='mse', optimizer=g_optim, metrics=['accuracy'])\n",
        "        d_on_g.compile(loss='binary_crossentropy', optimizer=g_optim,metrics=['accuracy'])\n",
        "    d.trainable = True\n",
        "    if reloadWeights == False:\n",
        "        #M#d.compile(loss='mse', optimizer=d_optim, metrics=['accuracy'])\n",
        "        d.compile(loss='binary_crossentropy', optimizer=d_optim, metrics=['accuracy'])\n",
        "     \n",
        "    if (not os.path.isfile(WEIGHTS_PATH+\"train-status.csv\")) or (reloadWeights == False):\n",
        "        #Initialize train status file\n",
        "        field_names = ['epoch', 'g-loss', 'g-acc', 'd-loss', 'd-acc']\n",
        "        with open(WEIGHTS_PATH+\"train-status.csv\",\"w\") as trainStatusFile:\n",
        "            tStatusLogger = csv.DictWriter(trainStatusFile, fieldnames=field_names)\n",
        "            tStatusLogger.writeheader()\n",
        "\n",
        "    bestDAcc = 0\n",
        "    bestGLoss = 100\n",
        "    for epoch in range(300):\n",
        "        print (\"Epoch is\", epoch)\n",
        "        #n_iter = int(X_train.shape[0]/BATCH_SIZE)\n",
        "        n_iter = int(X_train_generator.samples / BATCH_SIZE)\n",
        "        progress_bar = Progbar(target=n_iter)\n",
        "        histGLoss = []\n",
        "        histGAcc = []\n",
        "        histDLoss = []\n",
        "        histDAcc = []\n",
        "        for index in range(n_iter):\n",
        "            # create random noise -> U(0,1) 10 latent vectors\n",
        "            noise = np.random.uniform(0, 1, size=(BATCH_SIZE, 100))\n",
        " \n",
        "            # load real data & generate fake data\n",
        "            #image_batch = X_train[index*BATCH_SIZE:(index+1)*BATCH_SIZE]\n",
        "            image_batch = (X_train_generator.next().astype(np.float32) - 127.5) / 127.5\n",
        "            if (image_batch.shape[0] != BATCH_SIZE):\n",
        "                #Incomplete last batch, skip to next batch\n",
        "                image_batch = (X_train_generator.next().astype(np.float32) - 127.5) / 127.5\n",
        "                \n",
        "            generated_images = g.predict(noise, verbose=0)\n",
        "             \n",
        "            # visualize training results\n",
        "            if (epoch % 5 == 0) and (index== n_iter-1):\n",
        "                image = combine_images(generated_images)\n",
        "                image = image*127.5+127.5\n",
        "                cv2.imwrite(RESULTS_PATH+str(epoch)+\"_\"+str(index)+\".png\", image)\n",
        " \n",
        "            # attach label for training discriminator\n",
        "            X = np.concatenate((image_batch, generated_images))\n",
        "            y = np.array([1] * BATCH_SIZE + [0] * BATCH_SIZE)\n",
        "             \n",
        "            # training discriminator\n",
        "            d_loss = d.train_on_batch(X, y)\n",
        " \n",
        "            # training generator\n",
        "            d.trainable = False\n",
        "            g_loss = d_on_g.train_on_batch(noise, np.array([1] * BATCH_SIZE))\n",
        "            d.trainable = True\n",
        " \n",
        "            progress_bar.update(index+1, values=[('g-loss:',g_loss[0]), ('g-acc',g_loss[1]), ('d-loss:',d_loss[0]),('d-acc:',d_loss[1])])\n",
        "            histGLoss.append(g_loss[0])\n",
        "            histGAcc.append(g_loss[1])\n",
        "            histDLoss.append(d_loss[0])\n",
        "            histDAcc.append(d_loss[1])\n",
        " \n",
        "        #save train status\n",
        "        tStatus = {}\n",
        "        tStatus['epoch'] = epoch\n",
        "        tStatus['g-loss'] = sum(histGLoss)/len(histGLoss)\n",
        "        tStatus['g-acc'] = sum(histGAcc)/len(histGAcc)\n",
        "        tStatus['d-loss'] = sum(histDLoss)/len(histDLoss)\n",
        "        tStatus['d-acc'] = sum(histDAcc)/len(histDAcc)\n",
        "\n",
        "        with open(WEIGHTS_PATH+\"train-status.csv\",\"a\") as trainStatusFile:\n",
        "            tStatusLogger = csv.DictWriter(trainStatusFile, fieldnames=field_names)\n",
        "            tStatusLogger.writerow(tStatus)\n",
        "\n",
        "        # save weights for each epoch\n",
        "        g.save_weights(WEIGHTS_PATH+'generator.h5', True)\n",
        "        d.save_weights(WEIGHTS_PATH+'discriminator.h5', True)\n",
        "        d_on_g.save_weights(WEIGHTS_PATH+'gan.h5', True)\n",
        "        \n",
        "        d1 = tStatus['d-acc'] + (1-tStatus['g-loss'])\n",
        "        if d1 > bestDAcc:\n",
        "            bestDAcc = d1\n",
        "            with open(WEIGHTS_PATH+\"discriminator.best.txt\", \"w\") as file:\n",
        "                file.write(\"Epoch: {} discriminator best acc: {}\".format(epoch, d1))\n",
        "            d.save_weights(WEIGHTS_PATH+'discriminator.best.h5', True)\n",
        "            \n",
        "        if ((sum(histGLoss)/len(histGLoss)) < bestGLoss):\n",
        "            bestGLoss = (sum(histGLoss)/len(histGLoss))\n",
        "            with open(WEIGHTS_PATH+\"generator.best.txt\", \"w\") as file:\n",
        "                file.write(\"Epoch: {} generator best loss: {}\".format(epoch, bestGLoss))\n",
        "            g.save_weights(WEIGHTS_PATH+'generator.best.h5', True)\n",
        "            \n",
        "    return d, g\n",
        " \n",
        "### generate images\n",
        "def generate(BATCH_SIZE):\n",
        "    g = load_generator_model()\n",
        "    noise = np.random.uniform(0, 1, (BATCH_SIZE, 100))\n",
        "    generated_images = g.predict(noise)\n",
        "    return generated_images\n",
        " \n",
        "### anomaly loss function \n",
        "def sum_of_residual(y_true, y_pred):\n",
        "    return K.sum(K.abs(y_true - y_pred))\n",
        " \n",
        "### discriminator intermediate layer feautre extraction\n",
        "def feature_extractor(d=None):\n",
        "    if d is None:\n",
        "        d = load_discriminator_model()\n",
        "    intermidiate_model = Model(inputs=d.layers[0].input, outputs=d.layers[-7].output)\n",
        "    intermidiate_model.compile(loss='binary_crossentropy', optimizer='rmsprop')\n",
        "    return intermidiate_model\n",
        " \n",
        "### anomaly detection model define\n",
        "def anomaly_detector(g=None, d=None):\n",
        "    if g is None:\n",
        "        g = load_generator_model()\n",
        "    if d is None:\n",
        "        d = load_discriminator_model()\n",
        "        \n",
        "    intermidiate_model = feature_extractor(d)\n",
        "    intermidiate_model.trainable = False\n",
        "    g = Model(inputs=g.layers[1].input, outputs=g.layers[-1].output)\n",
        "    g.trainable = False\n",
        "    # Input layer cann't be trained. Add new layer as same size & same distribution\n",
        "    aInput = Input(shape=(100,))\n",
        "    gInput = Dense((100), trainable=True)(aInput)\n",
        "    gInput = Activation('sigmoid')(gInput)\n",
        "     \n",
        "    # G & D feature\n",
        "    G_out = g(gInput)\n",
        "    D_out= intermidiate_model(G_out)    \n",
        "    model = Model(inputs=aInput, outputs=[G_out, D_out])\n",
        "    model.compile(loss=sum_of_residual, loss_weights= [0.90, 0.10], optimizer='rmsprop')\n",
        "     \n",
        "    # batchnorm learning phase fixed (test) : make non trainable\n",
        "    K.set_learning_phase(0)\n",
        "     \n",
        "    return model\n",
        " \n",
        "### anomaly detection\n",
        "def compute_anomaly_score(model, x, iterations=500, d=None):\n",
        "    z = np.random.uniform(0, 1, size=(1, 100))\n",
        "     \n",
        "    intermidiate_model = feature_extractor(d)\n",
        "    d_x = intermidiate_model.predict(x)\n",
        " \n",
        "    # learning for changing latent\n",
        "    loss = model.fit(z, [x, d_x], batch_size=1, epochs=iterations, verbose=0)\n",
        "    similar_data, _ = model.predict(z)\n",
        "     \n",
        "    loss = loss.history['loss'][-1]\n",
        "     \n",
        "    return loss, similar_data"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDzARAccR9q-",
        "colab_type": "code",
        "outputId": "1b74a306-fadc-4889-8969-7d5490b19dc9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1565
        }
      },
      "source": [
        "print(\"generator_model:\")\n",
        "t1 = generator_model()\n",
        "print(t1.summary())\n",
        "\n",
        "print(\"discriminator_model:\")\n",
        "t1 = discriminator_model()\n",
        "print(t1.summary())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0617 01:14:16.767421 139634469906304 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0617 01:14:16.813783 139634469906304 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0617 01:14:16.829480 139634469906304 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0617 01:14:16.936831 139634469906304 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "generator_model:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0617 01:14:17.029087 139634469906304 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "W0617 01:14:20.248766 139634469906304 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "W0617 01:14:20.386857 139634469906304 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0617 01:14:20.402600 139634469906304 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 131072)            13238272  \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 131072)            524288    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_1 (LeakyReLU)    (None, 131072)            0         \n",
            "_________________________________________________________________\n",
            "reshape_1 (Reshape)          (None, 32, 32, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_1 (Conv2DTr (None, 64, 64, 64)        32832     \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 64, 64, 64)        36928     \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 64, 64, 64)        256       \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 64, 64, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_2 (Conv2DTr (None, 128, 128, 64)      16448     \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 128, 128, 1)       1601      \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 128, 128, 1)       0         \n",
            "=================================================================\n",
            "Total params: 13,850,625\n",
            "Trainable params: 13,588,353\n",
            "Non-trainable params: 262,272\n",
            "_________________________________________________________________\n",
            "None\n",
            "discriminator_model:\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         (None, 128, 128, 1)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 128, 128, 64)      1664      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_2 (LeakyReLU)    (None, 128, 128, 64)      0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 128, 128, 64)      0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 64, 64, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 64, 64, 128)       204928    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_3 (LeakyReLU)    (None, 64, 64, 128)       0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 64, 64, 128)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 32, 32, 128)       0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 131072)            0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 131073    \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 1)                 0         \n",
            "=================================================================\n",
            "Total params: 337,665\n",
            "Trainable params: 337,665\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCb-DTp_Gn6e",
        "colab_type": "text"
      },
      "source": [
        "## Train Ano-GAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bP25VAPhDevq",
        "colab_type": "code",
        "outputId": "b16047c5-c475-49c6-eac5-4edb8e4ad0eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "\n",
        "train_directory=BASE_PATH+\"train/\"\n",
        "validation_directory=BASE_PATH+\"test/\"\n",
        "\n",
        "train_datagen = ImageDataGenerator()\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    directory   = train_directory,       # this is the target directory\n",
        "    target_size = (128, 128, 1)[:-1],         # all images will be resized to 64x64\n",
        "    batch_size  = 128,\n",
        "    color_mode  = \"grayscale\",                    # We use a grayscale dataset\n",
        "    classes=[\"natural\"],\n",
        "    class_mode  = None                            # We do not need to get any label => Everything is healthy\n",
        ")  \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 10657 images belonging to 1 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0OmuUq9GyN5",
        "colab_type": "code",
        "outputId": "85b3d6c8-55ab-4201-9c22-6dc848c72f01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3508
        }
      },
      "source": [
        "#%debug\n",
        "Model_d, Model_g = train(128, train_generator)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch is 0\n",
            "83/83 [==============================] - 110s 1s/step - g-loss:: 1.2210 - g-acc: 0.4751 - d-loss:: 0.5744 - d-acc:: 0.7553\n",
            "Epoch is 1\n",
            "83/83 [==============================] - 99s 1s/step - g-loss:: 0.2372 - g-acc: 0.9000 - d-loss:: 0.5312 - d-acc:: 0.7294\n",
            "Epoch is 2\n",
            "83/83 [==============================] - 99s 1s/step - g-loss:: 0.4888 - g-acc: 0.8512 - d-loss:: 0.6350 - d-acc:: 0.6782\n",
            "Epoch is 3\n",
            "83/83 [==============================] - 99s 1s/step - g-loss:: 0.3417 - g-acc: 0.8881 - d-loss:: 0.5800 - d-acc:: 0.6653\n",
            "Epoch is 4\n",
            "83/83 [==============================] - 99s 1s/step - g-loss:: 0.3355 - g-acc: 0.9168 - d-loss:: 0.6326 - d-acc:: 0.6388\n",
            "Epoch is 5\n",
            "83/83 [==============================] - 99s 1s/step - g-loss:: 0.0764 - g-acc: 0.9986 - d-loss:: 0.4808 - d-acc:: 0.7814\n",
            "Epoch is 6\n",
            "83/83 [==============================] - 99s 1s/step - g-loss:: 0.0258 - g-acc: 0.9997 - d-loss:: 0.3744 - d-acc:: 0.8357\n",
            "Epoch is 7\n",
            "83/83 [==============================] - 99s 1s/step - g-loss:: 0.0573 - g-acc: 0.9912 - d-loss:: 0.4480 - d-acc:: 0.7789\n",
            "Epoch is 8\n",
            "83/83 [==============================] - 99s 1s/step - g-loss:: 0.0618 - g-acc: 0.9965 - d-loss:: 0.4794 - d-acc:: 0.7569\n",
            "Epoch is 9\n",
            "83/83 [==============================] - 99s 1s/step - g-loss:: 0.2609 - g-acc: 0.9540 - d-loss:: 0.6863 - d-acc:: 0.5602\n",
            "Epoch is 10\n",
            "83/83 [==============================] - 99s 1s/step - g-loss:: 0.1804 - g-acc: 0.9925 - d-loss:: 0.6309 - d-acc:: 0.6506\n",
            "Epoch is 11\n",
            "83/83 [==============================] - 99s 1s/step - g-loss:: 0.0624 - g-acc: 0.9999 - d-loss:: 0.5131 - d-acc:: 0.7301\n",
            "Epoch is 12\n",
            "83/83 [==============================] - 99s 1s/step - g-loss:: 0.0569 - g-acc: 0.9928 - d-loss:: 0.5035 - d-acc:: 0.7529\n",
            "Epoch is 13\n",
            "83/83 [==============================] - 99s 1s/step - g-loss:: 0.1988 - g-acc: 0.9667 - d-loss:: 0.5646 - d-acc:: 0.6821\n",
            "Epoch is 14\n",
            "83/83 [==============================] - 99s 1s/step - g-loss:: 0.2842 - g-acc: 0.9543 - d-loss:: 0.7172 - d-acc:: 0.5461\n",
            "Epoch is 15\n",
            "83/83 [==============================] - 99s 1s/step - g-loss:: 0.4076 - g-acc: 0.9079 - d-loss:: 0.7151 - d-acc:: 0.5207\n",
            "Epoch is 16\n",
            "83/83 [==============================] - 99s 1s/step - g-loss:: 0.3897 - g-acc: 0.9351 - d-loss:: 0.6900 - d-acc:: 0.5489\n",
            "Epoch is 17\n",
            "83/83 [==============================] - 99s 1s/step - g-loss:: 0.0912 - g-acc: 0.9931 - d-loss:: 0.4921 - d-acc:: 0.7853\n",
            "Epoch is 18\n",
            "83/83 [==============================] - 99s 1s/step - g-loss:: 0.0978 - g-acc: 0.9577 - d-loss:: 0.2834 - d-acc:: 0.8962\n",
            "Epoch is 19\n",
            "83/83 [==============================] - 99s 1s/step - g-loss:: 0.1557 - g-acc: 0.9811 - d-loss:: 0.5451 - d-acc:: 0.7288\n",
            "Epoch is 20\n",
            "83/83 [==============================] - 99s 1s/step - g-loss:: 0.0699 - g-acc: 0.9935 - d-loss:: 0.4804 - d-acc:: 0.7641\n",
            "Epoch is 21\n",
            "83/83 [==============================] - 99s 1s/step - g-loss:: 0.1282 - g-acc: 0.9832 - d-loss:: 0.5792 - d-acc:: 0.6883\n",
            "Epoch is 22\n",
            "83/83 [==============================] - 99s 1s/step - g-loss:: 0.2068 - g-acc: 0.9890 - d-loss:: 0.6404 - d-acc:: 0.6187\n",
            "Epoch is 23\n",
            "83/83 [==============================] - 99s 1s/step - g-loss:: 0.3342 - g-acc: 0.9088 - d-loss:: 0.7040 - d-acc:: 0.5689\n",
            "Epoch is 24\n",
            "83/83 [==============================] - 99s 1s/step - g-loss:: 0.2703 - g-acc: 0.9497 - d-loss:: 0.6626 - d-acc:: 0.6027\n",
            "Epoch is 25\n",
            "83/83 [==============================] - 99s 1s/step - g-loss:: 0.1630 - g-acc: 0.9965 - d-loss:: 0.6111 - d-acc:: 0.6450\n",
            "Epoch is 26\n",
            "83/83 [==============================] - 99s 1s/step - g-loss:: 0.3143 - g-acc: 0.9206 - d-loss:: 0.6752 - d-acc:: 0.5862\n",
            "Epoch is 27\n",
            "83/83 [==============================] - 99s 1s/step - g-loss:: 0.3293 - g-acc: 0.9426 - d-loss:: 0.6648 - d-acc:: 0.5877\n",
            "Epoch is 28\n",
            "83/83 [==============================] - 99s 1s/step - g-loss:: 0.2076 - g-acc: 0.9717 - d-loss:: 0.6234 - d-acc:: 0.6611\n",
            "Epoch is 29\n",
            "83/83 [==============================] - 99s 1s/step - g-loss:: 0.1036 - g-acc: 0.9868 - d-loss:: 0.5097 - d-acc:: 0.7721\n",
            "Epoch is 30\n",
            "83/83 [==============================] - 99s 1s/step - g-loss:: 0.1961 - g-acc: 0.9883 - d-loss:: 0.6561 - d-acc:: 0.5967\n",
            "Epoch is 31\n",
            "83/83 [==============================] - 99s 1s/step - g-loss:: 0.2570 - g-acc: 0.9727 - d-loss:: 0.6410 - d-acc:: 0.6021\n",
            "Epoch is 32\n",
            "83/83 [==============================] - 99s 1s/step - g-loss:: 0.1387 - g-acc: 0.9774 - d-loss:: 0.5645 - d-acc:: 0.7197\n",
            "Epoch is 33\n",
            "83/83 [==============================] - 99s 1s/step - g-loss:: 0.1151 - g-acc: 0.9939 - d-loss:: 0.5778 - d-acc:: 0.7056\n",
            "Epoch is 34\n",
            "83/83 [==============================] - 99s 1s/step - g-loss:: 0.2071 - g-acc: 0.9960 - d-loss:: 0.6525 - d-acc:: 0.5849\n",
            "Epoch is 35\n",
            "83/83 [==============================] - 99s 1s/step - g-loss:: 0.2588 - g-acc: 0.9788 - d-loss:: 0.6458 - d-acc:: 0.6046\n",
            "Epoch is 36\n",
            "83/83 [==============================] - 99s 1s/step - g-loss:: 0.2040 - g-acc: 0.9719 - d-loss:: 0.6059 - d-acc:: 0.6593\n",
            "Epoch is 37\n",
            "83/83 [==============================] - 99s 1s/step - g-loss:: 0.1691 - g-acc: 0.9948 - d-loss:: 0.5942 - d-acc:: 0.6380\n",
            "Epoch is 38\n",
            "83/83 [==============================] - 99s 1s/step - g-loss:: 0.2675 - g-acc: 0.9767 - d-loss:: 0.6271 - d-acc:: 0.6040\n",
            "Epoch is 39\n",
            "83/83 [==============================] - 99s 1s/step - g-loss:: 0.1664 - g-acc: 0.9849 - d-loss:: 0.5507 - d-acc:: 0.6898\n",
            "Epoch is 40\n",
            "83/83 [==============================] - 99s 1s/step - g-loss:: 0.1520 - g-acc: 0.9808 - d-loss:: 0.5744 - d-acc:: 0.6869\n",
            "Epoch is 41\n",
            "83/83 [==============================] - 99s 1s/step - g-loss:: 0.3115 - g-acc: 0.9251 - d-loss:: 0.6832 - d-acc:: 0.5693\n",
            "Epoch is 42\n",
            "83/83 [==============================] - 99s 1s/step - g-loss:: 0.3041 - g-acc: 0.9253 - d-loss:: 0.6479 - d-acc:: 0.5882\n",
            "Epoch is 43\n",
            "83/83 [==============================] - 99s 1s/step - g-loss:: 0.2424 - g-acc: 0.9883 - d-loss:: 0.6295 - d-acc:: 0.6033\n",
            "Epoch is 44\n",
            "83/83 [==============================] - 99s 1s/step - g-loss:: 0.1933 - g-acc: 0.9863 - d-loss:: 0.5900 - d-acc:: 0.6511\n",
            "Epoch is 45\n",
            "83/83 [==============================] - 99s 1s/step - g-loss:: 0.1062 - g-acc: 0.9840 - d-loss:: 0.4725 - d-acc:: 0.7741\n",
            "Epoch is 46\n",
            "83/83 [==============================] - 99s 1s/step - g-loss:: 0.1502 - g-acc: 0.9877 - d-loss:: 0.6167 - d-acc:: 0.6482\n",
            "Epoch is 47\n",
            "83/83 [==============================] - 99s 1s/step - g-loss:: 0.2046 - g-acc: 0.9881 - d-loss:: 0.6477 - d-acc:: 0.6050\n",
            "Epoch is 48\n",
            "83/83 [==============================] - 99s 1s/step - g-loss:: 0.2710 - g-acc: 0.9586 - d-loss:: 0.6440 - d-acc:: 0.5977\n",
            "Epoch is 49\n",
            "83/83 [==============================] - 99s 1s/step - g-loss:: 0.1643 - g-acc: 0.9960 - d-loss:: 0.5706 - d-acc:: 0.6648\n",
            "Epoch is 50\n",
            "83/83 [==============================] - 99s 1s/step - g-loss:: 0.1668 - g-acc: 0.9953 - d-loss:: 0.6083 - d-acc:: 0.6365\n",
            "Epoch is 51\n",
            "83/83 [==============================] - 99s 1s/step - g-loss:: 0.1756 - g-acc: 0.9960 - d-loss:: 0.5895 - d-acc:: 0.6431\n",
            "Epoch is 52\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.1544 - g-acc: 0.9886 - d-loss:: 0.5975 - d-acc:: 0.6585\n",
            "Epoch is 53\n",
            "83/83 [==============================] - 99s 1s/step - g-loss:: 0.2444 - g-acc: 0.9637 - d-loss:: 0.6409 - d-acc:: 0.6101\n",
            "Epoch is 54\n",
            "83/83 [==============================] - 99s 1s/step - g-loss:: 0.2371 - g-acc: 0.9669 - d-loss:: 0.6385 - d-acc:: 0.6137\n",
            "Epoch is 55\n",
            "83/83 [==============================] - 99s 1s/step - g-loss:: 0.2872 - g-acc: 0.9639 - d-loss:: 0.6688 - d-acc:: 0.5695\n",
            "Epoch is 56\n",
            "83/83 [==============================] - 99s 1s/step - g-loss:: 0.1547 - g-acc: 0.9896 - d-loss:: 0.5209 - d-acc:: 0.6993\n",
            "Epoch is 57\n",
            "83/83 [==============================] - 99s 1s/step - g-loss:: 0.1240 - g-acc: 0.9978 - d-loss:: 0.5644 - d-acc:: 0.6795\n",
            "Epoch is 58\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.2130 - g-acc: 0.9795 - d-loss:: 0.6040 - d-acc:: 0.6369\n",
            "Epoch is 59\n",
            "83/83 [==============================] - 99s 1s/step - g-loss:: 0.1560 - g-acc: 0.9841 - d-loss:: 0.5377 - d-acc:: 0.7045\n",
            "Epoch is 60\n",
            "83/83 [==============================] - 99s 1s/step - g-loss:: 0.1531 - g-acc: 0.9878 - d-loss:: 0.5843 - d-acc:: 0.6616\n",
            "Epoch is 61\n",
            "83/83 [==============================] - 99s 1s/step - g-loss:: 0.1556 - g-acc: 0.9904 - d-loss:: 0.5860 - d-acc:: 0.6691\n",
            "Epoch is 62\n",
            "83/83 [==============================] - 99s 1s/step - g-loss:: 0.2143 - g-acc: 0.9778 - d-loss:: 0.6277 - d-acc:: 0.6184\n",
            "Epoch is 63\n",
            "83/83 [==============================] - 99s 1s/step - g-loss:: 0.2687 - g-acc: 0.9550 - d-loss:: 0.6371 - d-acc:: 0.6055\n",
            "Epoch is 64\n",
            "83/83 [==============================] - 99s 1s/step - g-loss:: 0.2611 - g-acc: 0.9542 - d-loss:: 0.6540 - d-acc:: 0.5946\n",
            "Epoch is 65\n",
            "83/83 [==============================] - 99s 1s/step - g-loss:: 0.2214 - g-acc: 0.9763 - d-loss:: 0.6246 - d-acc:: 0.6179\n",
            "Epoch is 66\n",
            "83/83 [==============================] - 99s 1s/step - g-loss:: 0.2419 - g-acc: 0.9785 - d-loss:: 0.6279 - d-acc:: 0.6170\n",
            "Epoch is 67\n",
            "83/83 [==============================] - 99s 1s/step - g-loss:: 0.1865 - g-acc: 0.9773 - d-loss:: 0.5745 - d-acc:: 0.6721\n",
            "Epoch is 68\n",
            "83/83 [==============================] - 99s 1s/step - g-loss:: 0.1895 - g-acc: 0.9848 - d-loss:: 0.5816 - d-acc:: 0.6566\n",
            "Epoch is 69\n",
            "83/83 [==============================] - 99s 1s/step - g-loss:: 0.3086 - g-acc: 0.9182 - d-loss:: 0.6498 - d-acc:: 0.6027\n",
            "Epoch is 70\n",
            "83/83 [==============================] - 99s 1s/step - g-loss:: 0.2432 - g-acc: 0.9593 - d-loss:: 0.6277 - d-acc:: 0.6203\n",
            "Epoch is 71\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.2338 - g-acc: 0.9601 - d-loss:: 0.6295 - d-acc:: 0.6345\n",
            "Epoch is 72\n",
            "83/83 [==============================] - 99s 1s/step - g-loss:: 0.2441 - g-acc: 0.9711 - d-loss:: 0.6371 - d-acc:: 0.6073\n",
            "Epoch is 73\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.2064 - g-acc: 0.9883 - d-loss:: 0.5957 - d-acc:: 0.6377\n",
            "Epoch is 74\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.1051 - g-acc: 0.9992 - d-loss:: 0.4936 - d-acc:: 0.7393\n",
            "Epoch is 75\n",
            "83/83 [==============================] - 99s 1s/step - g-loss:: 0.1769 - g-acc: 0.9834 - d-loss:: 0.6105 - d-acc:: 0.6431\n",
            "Epoch is 76\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.2386 - g-acc: 0.9665 - d-loss:: 0.6181 - d-acc:: 0.6230\n",
            "Epoch is 77\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.1643 - g-acc: 0.9922 - d-loss:: 0.5867 - d-acc:: 0.6565\n",
            "Epoch is 78\n",
            "83/83 [==============================] - 99s 1s/step - g-loss:: 0.2462 - g-acc: 0.9712 - d-loss:: 0.6310 - d-acc:: 0.6160\n",
            "Epoch is 79\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.1556 - g-acc: 0.9881 - d-loss:: 0.5565 - d-acc:: 0.6872\n",
            "Epoch is 80\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.1956 - g-acc: 0.9874 - d-loss:: 0.6118 - d-acc:: 0.6404\n",
            "Epoch is 81\n",
            "83/83 [==============================] - 99s 1s/step - g-loss:: 0.2041 - g-acc: 0.9879 - d-loss:: 0.6354 - d-acc:: 0.6218\n",
            "Epoch is 82\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.2112 - g-acc: 0.9830 - d-loss:: 0.6282 - d-acc:: 0.6227\n",
            "Epoch is 83\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.2134 - g-acc: 0.9688 - d-loss:: 0.6172 - d-acc:: 0.6401\n",
            "Epoch is 84\n",
            "83/83 [==============================] - 99s 1s/step - g-loss:: 0.1490 - g-acc: 0.9934 - d-loss:: 0.5410 - d-acc:: 0.7034\n",
            "Epoch is 85\n",
            "83/83 [==============================] - 99s 1s/step - g-loss:: 0.2176 - g-acc: 0.9543 - d-loss:: 0.6208 - d-acc:: 0.6424\n",
            "Epoch is 86\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.2144 - g-acc: 0.9644 - d-loss:: 0.5822 - d-acc:: 0.6686\n",
            "Epoch is 87\n",
            "83/83 [==============================] - 99s 1s/step - g-loss:: 0.1214 - g-acc: 0.9947 - d-loss:: 0.5315 - d-acc:: 0.7136\n",
            "Epoch is 88\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.3515 - g-acc: 0.8665 - d-loss:: 0.6822 - d-acc:: 0.5791\n",
            "Epoch is 89\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.2037 - g-acc: 0.9737 - d-loss:: 0.6101 - d-acc:: 0.6479\n",
            "Epoch is 90\n",
            "83/83 [==============================] - 99s 1s/step - g-loss:: 0.2115 - g-acc: 0.9819 - d-loss:: 0.6195 - d-acc:: 0.6418\n",
            "Epoch is 91\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.2114 - g-acc: 0.9785 - d-loss:: 0.6203 - d-acc:: 0.6358\n",
            "Epoch is 92\n",
            "83/83 [==============================] - 99s 1s/step - g-loss:: 0.2635 - g-acc: 0.9583 - d-loss:: 0.6371 - d-acc:: 0.6173\n",
            "Epoch is 93\n",
            "83/83 [==============================] - 99s 1s/step - g-loss:: 0.2006 - g-acc: 0.9831 - d-loss:: 0.5630 - d-acc:: 0.6773\n",
            "Epoch is 94\n",
            "83/83 [==============================] - 98s 1s/step - g-loss:: 0.1126 - g-acc: 0.9825 - d-loss:: 0.4630 - d-acc:: 0.7746\n",
            "Epoch is 95\n",
            " 3/83 [>.............................] - ETA: 1:35 - g-loss:: 0.1482 - g-acc: 0.9948 - d-loss:: 0.4872 - d-acc:: 0.7669"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VzmQ6vnF8bmw",
        "colab_type": "text"
      },
      "source": [
        "## Anamoly Detection, Approach - 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhIzmfdfH7_T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def anomaly_detection(test_img, g=None, d=None):\n",
        "    model = anomaly_detector(g=g, d=d)\n",
        "    ano_score, similar_img = compute_anomaly_score(model, test_img.reshape(1, 256, 256, 1), iterations=500, d=d)\n",
        "\n",
        "    # anomaly area, 255 normalization\n",
        "    np_residual = test_img.reshape(256,256,1) - similar_img.reshape(256,256,1)\n",
        "    np_residual = (np_residual + 2)/4\n",
        "\n",
        "    np_residual = (255*np_residual).astype(np.uint8)\n",
        "    original_x = (test_img.reshape(256,256,1)*127.5+127.5).astype(np.uint8)\n",
        "    similar_x = (similar_img.reshape(256,256,1)*127.5+127.5).astype(np.uint8)\n",
        "\n",
        "    original_x_color = cv2.cvtColor(original_x, cv2.COLOR_GRAY2BGR)\n",
        "    residual_color = cv2.applyColorMap(np_residual, cv2.COLORMAP_JET)\n",
        "    show = cv2.addWeighted(original_x_color, 0.3, residual_color, 0.7, 0.)\n",
        "\n",
        "    return ano_score, original_x, similar_x, show\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebKDYKJE0_tj",
        "colab_type": "code",
        "outputId": "6587fec5-546c-4dd6-8d0d-ca1e6d50dba0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "#test_image_batch = (natural_validation_generator.next()[0:5].astype(np.float32) - 127.5) / 127.5\n",
        "#for image in test_image_batch:\n",
        "#    img_idx = 0\n",
        "#    start = cv2.getTickCount()\n",
        "#    score, qurey, pred, diff = anomaly_detection(image, Model_g, Model_d)\n",
        "#    time = (cv2.getTickCount() - start) / cv2.getTickFrequency() * 1000\n",
        "#    print ('techno image: %d : done'%( img_idx), 'Score: %.2f'%score, 'Time:%.2fms'%time)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "techno image: 0 : done Score: 12296.56 Time:9583.61ms\n",
            "techno image: 0 : done Score: 21023.97 Time:9535.10ms\n",
            "techno image: 0 : done Score: 18588.28 Time:9842.25ms\n",
            "techno image: 0 : done Score: 15228.32 Time:10356.36ms\n",
            "techno image: 0 : done Score: 19894.46 Time:10160.00ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJbOo8WvRyiu",
        "colab_type": "code",
        "outputId": "de832ed1-0843-47bb-d3ca-3ab4c2c6030b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "#test_image_batch = (techno_validation_generator.next()[0:5].astype(np.float32) - 127.5) / 127.5\n",
        "#for image in test_image_batch:\n",
        "#    img_idx = 0\n",
        "#    start = cv2.getTickCount()\n",
        "#    score, qurey, pred, diff = anomaly_detection(image, Model_g, Model_d)\n",
        "#    time = (cv2.getTickCount() - start) / cv2.getTickFrequency() * 1000\n",
        "#    print ('techno image: %d : done'%( img_idx), 'Score: %.2f'%score, 'Time:%.2fms'%time)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "techno image: 0 : done Score: 23380.92 Time:8456.15ms\n",
            "techno image: 0 : done Score: 12817.43 Time:8430.62ms\n",
            "techno image: 0 : done Score: 53128.41 Time:8832.68ms\n",
            "techno image: 0 : done Score: 8442.28 Time:9100.25ms\n",
            "techno image: 0 : done Score: 22049.61 Time:9794.49ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-xYDNGBo8MR",
        "colab_type": "code",
        "outputId": "55166806-bdba-4ab5-87f7-a7c7189b4c16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#test_image_batch.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5, 256, 256, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rG_6BKA711mj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import matplotlib.pyplot as plt\n",
        "#%matplotlib inline\n",
        "#\n",
        "#plt.figure(1, figsize=(3, 3))\n",
        "#plt.title('query image')\n",
        "#plt.imshow(qurey.reshape(64,64), cmap=plt.cm.gray)\n",
        "#\n",
        "#print(\"anomaly score : \", score)\n",
        "#plt.figure(2, figsize=(3, 3))\n",
        "#plt.title('generated similar image')\n",
        "#plt.imshow(pred.reshape(64,64), cmap=plt.cm.gray)\n",
        "#\n",
        "#plt.figure(3, figsize=(3, 3))\n",
        "#plt.title('anomaly detection')\n",
        "#plt.imshow(cv2.cvtColor(diff,cv2.COLOR_BGR2RGB))\n",
        "#plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRVY5YoM8h4h",
        "colab_type": "text"
      },
      "source": [
        "## Anamoly Detection, Approach - 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkpcSQbO6YCC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "def detect_defects(discriminator_model, validation_generator, threshold=0.5, verbose=1):\n",
        "\n",
        "        total_samples = validation_generator.samples\n",
        "        batch_size = validation_generator.batch_size\n",
        "\n",
        "        results = list()\n",
        "        labels = list()\n",
        "\n",
        "        if (verbose != 0):\n",
        "            progress_bar = Progbar(target=total_samples)\n",
        "\n",
        "        for _ in range(np.ceil(total_samples / batch_size).astype(np.int32)):\n",
        "\n",
        "            image_batch, lbls = validation_generator.next()\n",
        "\n",
        "            labels = np.append(labels, lbls.reshape(lbls.shape[0]))\n",
        "            image_batch = (image_batch.astype(np.float32) - 127.5) / 127.5\n",
        "\n",
        "            tmp_rslt = discriminator_model.predict(\n",
        "                x=image_batch,\n",
        "                batch_size=image_batch.shape[0],\n",
        "                verbose=0\n",
        "            )\n",
        "\n",
        "            if (verbose != 0):\n",
        "                progress_bar.add(image_batch.shape[0])\n",
        "\n",
        "            results = np.append(results, tmp_rslt.reshape(tmp_rslt.shape[0]))\n",
        "\n",
        "        results = [1 if x >= threshold else 0 for x in results]\n",
        "\n",
        "        tn, fp, fn, tp = confusion_matrix(labels, results).ravel()\n",
        "        print(\"tn:\", tn, \"fp:\", fp, \"fn:\", fn, \"tp:\", tp)\n",
        "\n",
        "        #################### NON DEFECT SITUATIONS ####################\n",
        "\n",
        "        # Probability of Detecting a Non-Defect: (tp / (tp + fn))\n",
        "        if ((tp + fn) != 0):\n",
        "            recall = tp / (tp + fn)\n",
        "        else:\n",
        "            recall = 0.0\n",
        "\n",
        "        # Probability of Correctly Detecting a Non-Defect: (tp / (tp + fp))\n",
        "\n",
        "        if ((tp + fp) != 0):\n",
        "            precision = tp / (tp + fp)\n",
        "        else:\n",
        "            precision = 0.0\n",
        "\n",
        "        ###################### DEFECT SITUATIONS ######################\n",
        "\n",
        "        # Probability of Detecting a Defect: (tn / (tn + fp))\n",
        "        if ((tn + fp) != 0):\n",
        "            specificity = tn / (tn + fp)\n",
        "        else:\n",
        "            specificity = 0.0\n",
        "\n",
        "        # Probability of Correctly Detecting a Defect: (tn / (tn + fn))\n",
        "        if ((tn + fn) != 0):\n",
        "            negative_predictive_value = tn / (tn + fn)\n",
        "        else:\n",
        "            negative_predictive_value = 0.0\n",
        "\n",
        "        return precision, recall, specificity, negative_predictive_value\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyqL-Qn768b6",
        "colab_type": "code",
        "outputId": "52047804-96f3-4d8d-f2a9-2b5b0f3cdae0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "# this is a similar generator, for validation data\n",
        "test_datagen = ImageDataGenerator()\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "    directory   = validation_directory,     # this is the target directory\n",
        "    target_size = (128, 128, 1)[:-1],         # all images will be resized to 64x64\n",
        "    batch_size  = 64,\n",
        "    color_mode  = \"grayscale\",              # We use a grayscale dataset\n",
        "    classes     = [\"natural\", \"techno\"],        # {'docs': 0, 'other': 1} => Needs to be enforced\n",
        "    class_mode  = 'binary'                  # We want to have binary labels for validation\n",
        ")\n",
        "\n",
        "\n",
        "#predict\n",
        "d = load_discriminator_model()\n",
        "precision, recall, specificity, negative_predictive_value = detect_defects(d, validation_generator,threshold=0.3)\n",
        "\n",
        "print(\"Probability of Detecting a Non-Defect             => Recall      (tp / (tp + fn)):\", recall)\n",
        "print(\"Probability of Correctly Detecting a Non-Defect   => Precision   (tp / (tp + fp)):\", precision)\n",
        "print(\"Probability of Detecting a Defect                 => Specificity (tn / (tn + fp)):\", specificity)\n",
        "print(\"Probability of Correctly Detecting a Defect       => NPV         (tn / (tn + fn)):\", negative_predictive_value)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0617 01:14:52.342506 139634469906304 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Found 114 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0617 01:14:52.356811 139634469906304 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "114/114 [==============================] - 5s 45ms/step\n",
            "tn: 57 fp: 7 fn: 23 tp: 27\n",
            "Probability of Detecting a Non-Defect             => Recall      (tp / (tp + fn)): 0.54\n",
            "Probability of Correctly Detecting a Non-Defect   => Precision   (tp / (tp + fp)): 0.7941176470588235\n",
            "Probability of Detecting a Defect                 => Specificity (tn / (tn + fp)): 0.890625\n",
            "Probability of Correctly Detecting a Defect       => NPV         (tn / (tn + fn)): 0.7125\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGVdexRGL8OJ",
        "colab_type": "code",
        "outputId": "f59ef9c6-dce0-4028-ea0d-ac4ab283204e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "validation_generator.class_indices"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'natural': 1, 'techno': 0}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_lmz7AXNoOm",
        "colab_type": "code",
        "outputId": "7f920764-c960-47d4-fc3e-c502d7dcd2eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 926
        }
      },
      "source": [
        "from numpy import newaxis\n",
        "folder=validation_directory+\"techno\"\n",
        "\n",
        "technoPredictions = []\n",
        "for filename in os.listdir(folder):\n",
        "    img = cv2.imread(os.path.join(folder,filename),0)\n",
        "    img = cv2.resize(img,(128,128))\n",
        "    img = (img.astype(np.float32) - 127.5) / 127.5\n",
        "    im2 = np.array([img])\n",
        "    im2 = im2[:,:,:,newaxis]\n",
        "    #print(im2.shape)\n",
        "    tmp_rslt = d.predict(x=im2,\n",
        "                batch_size=1,\n",
        "                verbose=0\n",
        "            )\n",
        "    technoPredictions.append(tmp_rslt[0][0])\n",
        "    print(filename, tmp_rslt[0][0])\n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "img_x_7552_y_22400.jpg 1.0\n",
            "img_x_6912_y_20608.jpg 0.9883857\n",
            "img_x_7296_y_22144.jpg 0.31642044\n",
            "img_x_15488_y_22528.jpg 0.9999914\n",
            "img_x_15616_y_22656.jpg 0.99998665\n",
            "img_x_7552_y_22144.jpg 1.0\n",
            "img_x_6528_y_19584.jpg 0.15875597\n",
            "img_x_6400_y_19584.jpg 0.20403388\n",
            "img_x_6656_y_19584.jpg 0.21504915\n",
            "img_x_7424_y_22272.jpg 1.0\n",
            "img_x_6656_y_19456.jpg 0.06714094\n",
            "img_x_7040_y_20480.jpg 0.9870509\n",
            "img_x_9728_y_21376.jpg 0.9757642\n",
            "img_x_6784_y_20736.jpg 0.1392769\n",
            "img_x_6528_y_19712.jpg 0.24852754\n",
            "img_x_6784_y_20480.jpg 0.13802381\n",
            "img_x_7552_y_22272.jpg 1.0\n",
            "img_x_9728_y_21504.jpg 0.92922115\n",
            "img_x_7040_y_20608.jpg 0.9585537\n",
            "img_x_7168_y_20480.jpg 0.6556935\n",
            "img_x_7296_y_22400.jpg 0.071307056\n",
            "img_x_15616_y_22528.jpg 0.9999937\n",
            "img_x_7296_y_20608.jpg 0.79291654\n",
            "img_x_15360_y_22656.jpg 0.063316256\n",
            "img_x_6784_y_20608.jpg 0.18692394\n",
            "img_x_7424_y_22144.jpg 1.0\n",
            "img_x_7680_y_22272.jpg 0.5158083\n",
            "img_x_9600_y_21504.jpg 0.8906618\n",
            "img_x_6656_y_19712.jpg 0.17712303\n",
            "img_x_15488_y_22656.jpg 0.99998844\n",
            "img_x_7168_y_20352.jpg 0.69118655\n",
            "img_x_6400_y_19712.jpg 0.42864797\n",
            "img_x_7168_y_20736.jpg 0.60555464\n",
            "img_x_7424_y_22400.jpg 1.0\n",
            "img_x_7168_y_20608.jpg 0.6852349\n",
            "img_x_9600_y_21376.jpg 0.94565016\n",
            "img_x_6912_y_20736.jpg 0.6344467\n",
            "img_x_6528_y_19456.jpg 0.065867215\n",
            "img_x_7296_y_20352.jpg 0.2427961\n",
            "img_x_6912_y_20352.jpg 0.5181704\n",
            "img_x_6784_y_20352.jpg 0.14773753\n",
            "img_x_7296_y_22272.jpg 0.029392548\n",
            "img_x_7680_y_22400.jpg 0.09613475\n",
            "img_x_7680_y_22144.jpg 0.35092196\n",
            "img_x_7040_y_20736.jpg 0.9690745\n",
            "img_x_7040_y_20352.jpg 0.64472204\n",
            "img_x_6912_y_20480.jpg 0.88757324\n",
            "img_x_7296_y_20736.jpg 0.66728276\n",
            "img_x_15360_y_22528.jpg 0.1101865\n",
            "img_x_7296_y_20480.jpg 0.8406859\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0-PAWnmnaBU",
        "colab_type": "code",
        "outputId": "1c4e0446-d770-42e0-fe97-34ed02a7d786",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1181
        }
      },
      "source": [
        "from numpy import newaxis\n",
        "folder=validation_directory+\"natural\"\n",
        "\n",
        "naturalPredictions = []\n",
        "for filename in os.listdir(folder):\n",
        "    img = cv2.imread(os.path.join(folder,filename),0)\n",
        "    img = cv2.resize(img,(128,128))\n",
        "    img = (img.astype(np.float32) - 127.5) / 127.5\n",
        "    im2 = np.array([img])\n",
        "    im2 = im2[:,:,:,newaxis]\n",
        "    #print(im2.shape)\n",
        "    tmp_rslt = d.predict(x=im2,\n",
        "                batch_size=1,\n",
        "                verbose=0\n",
        "            )\n",
        "    naturalPredictions.append(tmp_rslt[0][0])\n",
        "    print(filename, tmp_rslt[0][0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "img_x_10752_y_17408.jpg 0.14697611\n",
            "img_x_11648_y_16896.jpg 0.19080912\n",
            "img_x_13568_y_7808.jpg 0.14173661\n",
            "img_x_14336_y_18688.jpg 0.22423638\n",
            "img_x_9472_y_29184.jpg 0.9882239\n",
            "img_x_10368_y_8960.jpg 0.20709567\n",
            "img_x_11648_y_21504.jpg 0.019217575\n",
            "img_x_14720_y_8448.jpg 0.3724327\n",
            "img_x_11520_y_9984.jpg 0.36900547\n",
            "img_x_14720_y_8192.jpg 0.074692704\n",
            "img_x_12288_y_13440.jpg 0.4571508\n",
            "img_x_10752_y_7552.jpg 0.26875517\n",
            "img_x_13824_y_17408.jpg 0.06876851\n",
            "img_x_9728_y_10880.jpg 0.1346322\n",
            "img_x_9856_y_18176.jpg 0.014412622\n",
            "img_x_9856_y_22016.jpg 0.13594581\n",
            "img_x_11648_y_9984.jpg 0.6636361\n",
            "img_x_10240_y_25344.jpg 0.27711833\n",
            "img_x_12672_y_18176.jpg 0.098404996\n",
            "img_x_14976_y_13184.jpg 0.059294216\n",
            "img_x_11264_y_29440.jpg 0.9999976\n",
            "img_x_9216_y_28544.jpg 0.3732963\n",
            "img_x_9472_y_9728.jpg 0.5726497\n",
            "img_x_9472_y_6784.jpg 0.14794983\n",
            "img_x_9600_y_9088.jpg 0.21641523\n",
            "img_x_12288_y_29824.jpg 0.9999727\n",
            "img_x_12032_y_18688.jpg 0.091706984\n",
            "img_x_8960_y_6144.jpg 0.08773631\n",
            "img_x_11776_y_13056.jpg 0.045334373\n",
            "img_x_9216_y_7424.jpg 0.30454227\n",
            "img_x_8064_y_10240.jpg 0.20983566\n",
            "img_x_8832_y_18688.jpg 0.011863927\n",
            "img_x_12160_y_13440.jpg 0.36512658\n",
            "img_x_8960_y_28032.jpg 0.14859019\n",
            "img_x_12800_y_17408.jpg 0.023274183\n",
            "img_x_11392_y_17792.jpg 0.15377447\n",
            "img_x_9728_y_10368.jpg 0.56292665\n",
            "img_x_8320_y_9984.jpg 0.07900329\n",
            "img_x_12032_y_7808.jpg 0.47170514\n",
            "img_x_11136_y_21888.jpg 0.3286\n",
            "img_x_11904_y_8192.jpg 0.24225633\n",
            "img_x_9984_y_25728.jpg 0.1899763\n",
            "img_x_11264_y_5248.jpg 0.25396875\n",
            "img_x_14720_y_5888.jpg 0.13787867\n",
            "img_x_8064_y_13184.jpg 0.07925973\n",
            "img_x_9600_y_20224.jpg 0.044926874\n",
            "img_x_9600_y_17024.jpg 0.13444531\n",
            "img_x_11648_y_10368.jpg 0.050922308\n",
            "img_x_10752_y_6784.jpg 0.44476157\n",
            "img_x_9216_y_25984.jpg 0.14460796\n",
            "img_x_14592_y_19456.jpg 0.30598563\n",
            "img_x_11392_y_13696.jpg 0.23397698\n",
            "img_x_9856_y_20736.jpg 0.5714079\n",
            "img_x_11520_y_7680.jpg 0.15653525\n",
            "img_x_12032_y_22144.jpg 0.09517314\n",
            "img_x_12544_y_5248.jpg 0.16616157\n",
            "img_x_12160_y_22272.jpg 0.2805331\n",
            "img_x_8576_y_8960.jpg 0.2444867\n",
            "img_x_11648_y_27520.jpg 0.6211655\n",
            "img_x_8192_y_29824.jpg 0.32684308\n",
            "img_x_9984_y_7424.jpg 0.14255606\n",
            "img_x_13440_y_18688.jpg 0.0108824\n",
            "img_x_10240_y_26240.jpg 0.088054284\n",
            "img_x_12416_y_7808.jpg 0.15235864\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjklyXcJ8JA4",
        "colab_type": "code",
        "outputId": "932ea21c-f65d-4444-d78d-d769c2bd9364",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        }
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.DataFrame({'natural':naturalPredictions[:50],'techno':technoPredictions})\n",
        "df.boxplot(column=['natural','techno'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7efe7140f4e0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGmS4S1KD/g8F1l8H5NPEvQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4qg-84mQgCh",
        "colab_type": "code",
        "outputId": "7263500d-39fe-476f-d46c-b5df4051273e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('img_x_10752_y_17408.jpg', 0.012893274)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVLE89me9bMn",
        "colab_type": "code",
        "outputId": "1926a827-0c5a-4300-fd43-b55b5f1b489e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        }
      },
      "source": [
        "df.hist()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x7efe713449e8>,\n",
              "        <matplotlib.axes._subplots.AxesSubplot object at 0x7efe713029e8>]],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEICAYAAABPgw/pAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NiBbTrnBRMS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}